import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from project1 import CMUMOSEIDataset   # æ›¿æ¢æˆä½ çš„ Dataset æ–‡ä»¶
from test_mosi import MFN              # æ›¿æ¢æˆä½ çš„æ¨¡å‹æ–‡ä»¶
import pickle

# ========== åŠ è½½æ•°æ® ==========
with open("cmumosei_highlevel.pkl", "rb") as f:
    cmumosei_highlevel = pickle.load(f)

# ========== å‚æ•°é…ç½® ==========
batch_size = 32
num_epochs = 10
learning_rate = 1e-5
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ========== æ•°æ®åŠ è½½ ==========
dataset = CMUMOSEIDataset(cmumosei_highlevel)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# ========== æ¨¡å‹é…ç½® ==========
config = {
    "input_dims": [300, 74, 713],
    "h_dims": [128, 32, 32],
    "memsize": 64,
    "windowsize": 2,
    "output_dim": 2
}
NN1Config = {"shapes": 128, "drop": 0.3}
NN2Config = {"shapes": 64, "drop": 0.3}
gamma1Config = {"shapes": 64, "drop": 0.3}
gamma2Config = {"shapes": 64, "drop": 0.3}
outConfig = {"shapes": 32, "drop": 0.3}

model = MFN(config, NN1Config, NN2Config, gamma1Config, gamma2Config, outConfig).to(device)

# ========== æŸå¤±å‡½æ•° & ä¼˜åŒ–å™¨ ==========
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

print("ğŸ” å¼€å§‹è®­ç»ƒ MFN æ¨¡å‹...")

for epoch in range(num_epochs):
    model.train()
    total_loss = 0.0

    for batch_idx, ((text, audio, visual), y) in enumerate(dataloader):
        text = text.permute(1, 0, 2).to(device)   # (T, B, D_text)
        audio = audio.permute(1, 0, 2).to(device)  # (T, B, D_audio)
        visual = visual.permute(1, 0, 2).to(device)  # (T, B, D_visual)
        y = (y > 0).long().to(device)  # (B,)

        # ğŸ› ï¸ ğŸ”¥ åŠ æ£€æŸ¥ NaN çš„ä»£ç åœ¨è¿™é‡Œ
        if torch.isnan(text).any():
            print(f"[Batch {batch_idx}] NaN in text input!")
        if torch.isnan(audio).any():
            print(f"[Batch {batch_idx}] NaN in audio input!")
        if torch.isnan(visual).any():
            print(f"[Batch {batch_idx}] NaN in visual input!")
        if torch.isnan(y).any():
            print(f"[Batch {batch_idx}] NaN in labels!")

        optimizer.zero_grad()

        output = model(text, audio, visual)  # ä¼ ä¸‰ä¸ªæ¨¡æ€è¿›å»
        # ğŸ› ï¸ ğŸ”¥ æ£€æŸ¥ output æœ‰æ²¡æœ‰ NaN
        if torch.isnan(output).any():
            print(f"[Batch {batch_idx}] NaN in model output!")
        loss = criterion(output, y)

        if torch.isnan(loss):
            print(f"[Warning] Loss is NaN at batch {batch_idx}, skipping...")
            continue

        loss.backward()

        # ğŸš¨ åŠ æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)

        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(dataloader)
    print(f"[Epoch {epoch+1}/{num_epochs}] Loss: {avg_loss:.4f}")

# ========== ä¿å­˜æ¨¡å‹ ==========
torch.save(model.state_dict(), "mfn_cls_model.pt")
print("âœ… æ¨¡å‹å·²ä¿å­˜ä¸º mfn_model.pt")

